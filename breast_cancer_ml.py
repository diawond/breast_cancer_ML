# -*- coding: utf-8 -*-
"""breast_cancer_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-y3zxtdJVsAF1kUvYAm5RcBQAAtIPqa
"""

pip install kagglehub pandas scikit-learn matplotlib seaborn

import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import silhouette_score

# 1. ดาวน์โหลดและโหลดข้อมูล
# path ที่ได้จะเป็น directory ที่เก็บไฟล์ข้อมูล
path = kagglehub.dataset_download("uciml/breast-cancer-wisconsin-data")
file_path = f"{path}/data.csv"
df = pd.read_csv(file_path)

# 2. การเตรียมข้อมูล (Data Preprocessing)
# ลบคอลัมน์ที่ไม่จำเป็นออก
df = df.drop(['id', 'Unnamed: 32'], axis=1)

# แปลงค่าในคอลัมน์ 'diagnosis' (เป้าหมาย) ให้เป็นตัวเลข
# 'M' (Malignant) -> 1
# 'B' (Benign) -> 0
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

# 3. แยกข้อมูลเป็น Features (X) และ Target (y)
X = df.drop('diagnosis', axis=1)
y = df['diagnosis']

# 4. แบ่งข้อมูลสำหรับ Train และ Test
# stratify=y ช่วยให้สัดส่วนของ class 0 และ 1 ใน train/test set ใกล้เคียงกับข้อมูลจริง
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

"""Decision Tree (การทำนาย/การจำแนกประเภท)"""

### Decision Tree Classifier with GridSearchCV

# 1. กำหนดช่วงของ Hyperparameters ที่ต้องการค้นหา
param_grid_dt = {
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

# 2. สร้างและรัน GridSearchCV เพื่อหาพารามิเตอร์ที่ดีที่สุด
grid_search_dt = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                              param_grid=param_grid_dt,
                              cv=5,
                              n_jobs=-1,
                              verbose=1)

print("\nกำลังเริ่มการค้นหา Hyperparameter สำหรับ Decision Tree...")
grid_search_dt.fit(X_train, y_train)

# 3. แสดงผลลัพธ์และประเมินโมเดล
print("\nBest Parameters found:", grid_search_dt.best_params_)

best_dt_model = grid_search_dt.best_estimator_
y_pred_dt = best_dt_model.predict(X_test)

print("\nClassification Report (Decision Tree):")
print(classification_report(y_test, y_pred_dt, target_names=['Benign', 'Malignant']))

# 4. แสดง Confusion Matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.title('Confusion Matrix - Decision Tree')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# 5. แสดงผล Decision Tree เป็นภาพ
plt.figure(figsize=(20, 15))
plot_tree(best_dt_model, filled=True, feature_names=X.columns, class_names=['Benign', 'Malignant'], fontsize=10)
plt.title("Best Decision Tree Model")
plt.show()

""" Random Forest (การทำนาย/การจำแนกประเภท)"""

### Random Forest Classifier with GridSearchCV

# 1. กำหนดช่วงของ Hyperparameters ที่ต้องการค้นหา
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'criterion': ['gini', 'entropy']
}

# 2. สร้างและรัน GridSearchCV
grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                              param_grid=param_grid_rf,
                              cv=5,
                              n_jobs=-1,
                              verbose=1)

print("\nกำลังเริ่มการค้นหา Hyperparameter สำหรับ Random Forest...")
grid_search_rf.fit(X_train, y_train)

# 3. แสดงผลลัพธ์และประเมินโมเดล
print("\nBest Parameters found:", grid_search_rf.best_params_)

best_rf_model = grid_search_rf.best_estimator_
y_pred_rf = best_rf_model.predict(X_test)

print("\nClassification Report (Random Forest):")
print(classification_report(y_test, y_pred_rf, target_names=['Benign', 'Malignant']))

# 4. แสดง Confusion Matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# 5. แสดง Feature Importance
feature_importances = pd.Series(best_rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(12, 8))
sns.barplot(x=feature_importances.head(10), y=feature_importances.head(10).index, palette='viridis')
plt.title('Top 10 Feature Importances')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.show()

### Logistic Regression with Pipeline and GridSearchCV

# 1. สร้าง Pipeline (รวมขั้นตอน StandardScaler และ LogisticRegression)
pipeline_lr = Pipeline([
    ('scaler', StandardScaler()),
    ('logreg', LogisticRegression(solver='liblinear', random_state=42))
])

# 2. กำหนด Hyperparameters ที่จะค้นหา
param_grid_lr = {
    'logreg__penalty': ['l1', 'l2'],
    'logreg__C': [0.01, 0.1, 1, 10, 100] # C คือค่า Regularization strength
}

# 3. สร้างและรัน GridSearchCV
grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=5, verbose=1, n_jobs=-1)

print("\nกำลังเริ่มการค้นหา Hyperparameter สำหรับ Logistic Regression...")
grid_search_lr.fit(X_train, y_train)

# 4. แสดงผลลัพธ์และประเมินโมเดล
print("\nBest Parameters found:", grid_search_lr.best_params_)

best_lr_model = grid_search_lr.best_estimator_
y_pred_lr = best_lr_model.predict(X_test)

print("\nClassification Report (Logistic Regression):")
print(classification_report(y_test, y_pred_lr, target_names=['Benign', 'Malignant']))

# 5. แสดง Confusion Matrix
cm_lr = confusion_matrix(y_test, y_pred_lr)
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.title('Confusion Matrix - Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

### KNN Classifier with Pipeline and GridSearchCV

# 1. สร้าง Pipeline (รวมขั้นตอน StandardScaler และ KNN)
pipeline_knn = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier())
])

# 2. กำหนด Hyperparameters ที่จะค้นหา
param_grid_knn = {
    'knn__n_neighbors': list(range(1, 31)), # ทดสอบค่า k ตั้งแต่ 1 ถึง 30
    'knn__weights': ['uniform', 'distance']
}

# 3. สร้างและรัน GridSearchCV
grid_search_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv=5, verbose=1, n_jobs=-1)

print("\nกำลังเริ่มการค้นหา Hyperparameter สำหรับ KNN...")
grid_search_knn.fit(X_train, y_train)

# 4. แสดงผลลัพธ์และประเมินโมเดล
print("\nBest Parameters found:", grid_search_knn.best_params_)

best_knn_model = grid_search_knn.best_estimator_
y_pred_knn = best_knn_model.predict(X_test)

print("\nClassification Report (KNN):")
print(classification_report(y_test, y_pred_knn, target_names=['Benign', 'Malignant']))

# 5. แสดง Confusion Matrix
cm_knn = confusion_matrix(y_test, y_pred_knn)
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Purples', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.title('Confusion Matrix - KNN')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

### K-Means Clustering with Elbow Method

# 1. เตรียมข้อมูลสำหรับ Clustering (ต้องทำ Scaling ก่อน)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. ใช้วิธี Elbow Method เพื่อหาจำนวน Clusters (K) ที่เหมาะสม
wcss = []  # Within-Cluster Sum of Squares
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# แสดงผลกราฟ Elbow Method
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.show()

# 3. สร้างโมเดล K-Means โดยเลือก K ที่เหมาะสมจากกราฟ (ในที่นี้คือ K=2)
# จากกราฟจะเห็นจุดหักศอกที่ K=2 ซึ่งสอดคล้องกับจำนวนคลาสในชุดข้อมูล
kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=42)
kmeans.fit(X_scaled)
clusters = kmeans.labels_

# 4. ประเมินผล Clustering
# เราจะใช้ Silhouette Score เพื่อประเมินว่ากลุ่มที่ได้มีความแน่นหนาแค่ไหน
silhouette_avg = silhouette_score(X_scaled, clusters)
print(f"\nSilhouette Score: {silhouette_avg:.2f}")

# Cross-tabulation: เพื่อดูว่าผลลัพธ์ของ K-Means สอดคล้องกับคลาสจริงหรือไม่
clustered_df = pd.DataFrame({'Cluster': clusters, 'Actual Diagnosis': y})
comparison_table = pd.crosstab(clustered_df['Cluster'], clustered_df['Actual Diagnosis'])
comparison_table.columns = ['Benign', 'Malignant']
comparison_table.index = ['Cluster 0', 'Cluster 1']
print("\nComparison of K-Means Clusters with Actual Diagnosis:")
print(comparison_table)